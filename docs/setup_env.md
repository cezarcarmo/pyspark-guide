# ConfiguraÃ§Ã£o do Ambiente PySpark

Este documento detalha os passos para configurar um ambiente de desenvolvimento PySpark utilizando **Conda**.

## ğŸ”¥ Criando o Ambiente Virtual com Conda

1. **Criar um ambiente Conda chamado `pyspark-env`**:
   ```bash
   conda create --name pyspark-env python=3.8
   ```
2. **Ativar o ambiente**:
   ```bash
   conda activate pyspark-env
   ```
3. **Instalar as dependÃªncias**:
   ```bash
   conda install -c conda-forge pyspark jupyter findspark
   ```

## ğŸ–¥ ConfiguraÃ§Ã£o no VSCode

1. Abra o **VSCode** e instale as extensÃµes:
   - **Python**
   - **Jupyter**
2. Selecione o ambiente virtual no VSCode:
   - Pressione `Ctrl+Shift+P`
   - Pesquise **Python: Select Interpreter**
   - Escolha `pyspark-env` criado com Conda

## ğŸš€ Testando a ConfiguraÃ§Ã£o com Jupyter Notebook

1. **Instalar o Jupyter Notebook no ambiente**:
   ```bash
   pip install jupyter
   ```
2. **Abrir o Jupyter Notebook**:
   ```bash
   jupyter notebook
   ```
3. **Criar um novo notebook e rodar o cÃ³digo abaixo para testar a integraÃ§Ã£o**:
   ```python
   import findspark
   findspark.init()

   from pyspark.sql import SparkSession

   spark = SparkSession.builder.appName("TestePySpark").getOrCreate()
   print(spark.version)
   ```

Se a versÃ£o do Spark for exibida sem erros, seu ambiente estÃ¡ pronto! ğŸ‰

