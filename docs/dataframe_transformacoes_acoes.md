# Transforma√ß√µes e A√ß√µes em DataFrames

Nesta se√ß√£o, exploramos transforma√ß√µes e a√ß√µes aplic√°veis a **DataFrames** no PySpark. Transforma√ß√µes s√£o opera√ß√µes que geram novos DataFrames, enquanto a√ß√µes executam computa√ß√µes e retornam resultados.

---

## üîÑ **Transforma√ß√µes em DataFrames**

### 1Ô∏è‚É£ **Adicionar uma Nova Coluna com `withColumn`**
- Usado para criar ou modificar colunas no DataFrame.

```python
from pyspark.sql.functions import col

# Adicionar uma nova coluna chamada 'idade_em_2025'
df = df.withColumn("idade_em_2025", col("idade") + 2)
df.show()
```

### 2Ô∏è‚É£ **Remover uma Coluna com `drop`**
- Remove colunas desnecess√°rias do DataFrame.

```python
# Remover a coluna 'idade_em_2025'
df = df.drop("idade_em_2025")
df.show()
```

### 3Ô∏è‚É£ **Selecionar Valores √önicos com `distinct`**
- Retorna apenas os valores √∫nicos de um DataFrame.

```python
# Selecionar valores √∫nicos com base na coluna 'idade'
df.distinct().show()
```

### 4Ô∏è‚É£ **Renomear Colunas**
- Renomeia colunas do DataFrame.

```python
# Renomear a coluna 'nome' para 'nome_completo'
df = df.withColumnRenamed("nome", "nome_completo")
df.show()
```

---

## üõ† **A√ß√µes em DataFrames**

### 1Ô∏è‚É£ **Contar Registros com `count`**
- Retorna o n√∫mero total de registros no DataFrame.

```python
# Contar o n√∫mero de registros no DataFrame
print("N√∫mero de registros:", df.count())
```

### 2Ô∏è‚É£ **Coletar Dados Localmente com `collect`**
- Traz todos os registros para o driver como uma lista (use com cuidado para grandes DataFrames).

```python
# Coletar os dados do DataFrame para uma lista
resultados = df.collect()
for linha in resultados:
    print(linha)
```

### 3Ô∏è‚É£ **Exibir Amostras com `show` e `take`**
- `show` exibe os dados em formato tabular no console.
- `take` retorna um n√∫mero especificado de registros como uma lista.

```python
# Exibir os 5 primeiros registros
df.show(5)

# Pegar os 3 primeiros registros
df.take(3)
```

---

## üåü **Exemplo Completo**
### C√≥digo para Testar:

1. Cria√ß√£o de DataFrame:
   ```python
   from pyspark.sql import SparkSession
   spark = SparkSession.builder.appName("TransformacoesAcoes").getOrCreate()

   # Criar DataFrame
data = [(1, "Ana", 25), (2, "Carlos", 30), (3, "Jo√£o", 45), (4, "Mariana", 35)]
columns = ["id", "nome", "idade"]
df = spark.createDataFrame(data, columns)
   ```

2. Aplicar Transforma√ß√µes:
   ```python
   from pyspark.sql.functions import col

   # Adicionar uma nova coluna
df = df.withColumn("idade_em_2025", col("idade") + 2)
   ```

3. Executar A√ß√µes:
   ```python
   # Contar registros
   print("N√∫mero de registros:", df.count())

   # Exibir DataFrame
   df.show()
   ```

---

## üìÑ **Conclus√£o**

- As transforma√ß√µes em DataFrames criam novos DataFrames sem alterar os originais.
- A√ß√µes executam opera√ß√µes e retornam resultados ao driver.
- Combinando ambas, √© poss√≠vel manipular dados de maneira eficiente no PySpark.

Para mais informa√ß√µes, consulte a [documenta√ß√£o oficial do PySpark SQL](https://spark.apache.org/docs/latest/sql-programming-guide.html).

