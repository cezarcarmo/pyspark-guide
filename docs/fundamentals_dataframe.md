# Fundamentos do PySpark: DataFrames

Nesta se√ß√£o, exploramos os conceitos fundamentais de **DataFrames** no PySpark. Os DataFrames s√£o uma abstra√ß√£o mais poderosa que os RDDs, otimizados para consultas e opera√ß√µes estruturadas em grandes volumes de dados.

---

## üî• **O que √© um DataFrame?**

- Um **DataFrame** √© uma cole√ß√£o distribu√≠da de dados organizados em colunas, semelhante a uma tabela em um banco de dados relacional ou um dataframe do Pandas.
- Os DataFrames s√£o imut√°veis e distribuem automaticamente as opera√ß√µes entre n√≥s do cluster Spark.
- Eles permitem:
  - Opera√ß√µes SQL diretamente no Spark.
  - Otimiza√ß√µes autom√°ticas com o **Catalyst Optimizer**.

---

## üõ† **Criando um DataFrame no PySpark**

### 1Ô∏è‚É£ **Criar um DataFrame a partir de uma lista**

```python
from pyspark.sql import SparkSession

# Criar SparkSession
spark = SparkSession.builder.appName("DataFrameExample").getOrCreate()

# Dados de exemplo
data = [(1, "Ana", 25), (2, "Carlos", 30), (3, "Jo√£o", 45), (4, "Mariana", 35)]
columns = ["id", "nome", "idade"]

# Criar o DataFrame
df = spark.createDataFrame(data, columns)

# Exibir o DataFrame
df.show()
```

Sa√≠da esperada:
```
+---+--------+-----+
| id|    nome|idade|
+---+--------+-----+
|  1|     Ana|   25|
|  2|  Carlos|   30|
|  3|    Jo√£o|   45|
|  4| Mariana|   35|
+---+--------+-----+
```

---

### 2Ô∏è‚É£ **Criar um DataFrame a partir de um arquivo CSV**

```python
# Ler um arquivo CSV
file_path = "data/exemplo.csv"
df_csv = spark.read.csv(file_path, header=True, inferSchema=True)

# Exibir o DataFrame
df_csv.show()
```

---

## üöÄ **Opera√ß√µes B√°sicas com DataFrames**

### **1. Sele√ß√£o de Colunas**
```python
# Selecionar uma coluna espec√≠fica
df.select("nome").show()

# Selecionar v√°rias colunas
df.select("nome", "idade").show()
```

### **2. Filtrar Linhas**
```python
# Filtrar linhas onde a idade seja maior que 30
df.filter(df["idade"] > 30).show()
```

### **3. Ordenar Dados**
```python
# Ordenar os dados por idade (ascendente)
df.orderBy("idade").show()

# Ordenar por idade (descendente)
df.orderBy(df["idade"].desc()).show()
```

### **4. Agrega√ß√µes**
```python
from pyspark.sql.functions import avg, max

# Calcular a idade m√©dia
df.select(avg("idade").alias("idade_media")).show()

# Encontrar a maior idade
df.select(max("idade").alias("maior_idade")).show()
```

---

## üîÑ **Compara√ß√£o: RDDs vs DataFrames**

| **Aspecto**       | **RDDs**                          | **DataFrames**                     |
|-------------------|-----------------------------------|------------------------------------|
| **Abstra√ß√£o**     | Dados distribu√≠dos simples        | Dados estruturados (tabelas)       |
| **Performance**   | Menos otimizado                  | Otimizado com Catalyst Optimizer   |
| **Facilidade**    | Programa√ß√£o manual               | Opera√ß√µes SQL-like f√°ceis          |
| **Quando usar**   | Processamento n√£o estruturado     | Dados estruturados e semi-estruturados |

---

## üìÑ **Conclus√£o**

- Os DataFrames s√£o mais f√°ceis de usar e muito mais otimizados do que os RDDs para processamento de dados estruturados.
- Permitem opera√ß√µes complexas como agrega√ß√µes, ordena√ß√µes e filtros com pouca complexidade de c√≥digo.

Para mais detalhes, consulte a [documenta√ß√£o oficial do PySpark SQL](https://spark.apache.org/docs/latest/sql-programming-guide.html).

