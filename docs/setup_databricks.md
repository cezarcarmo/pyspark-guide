# Configura√ß√£o do PySpark no Databricks

Nesta se√ß√£o, configuraremos o **Databricks** para rodar PySpark, criando um cluster e executando notebooks no ambiente de nuvem.

---

## üìå **O que √© o Databricks?**

O **Databricks** √© uma plataforma unificada baseada no **Apache Spark**, otimizada para an√°lise de dados, engenharia de dados e aprendizado de m√°quina. Ele oferece um ambiente gerenciado, eliminando a necessidade de configurar e manter infraestrutura manualmente.

---

## üöÄ **Passo a Passo para Configura√ß√£o do Databricks**

### 1Ô∏è‚É£ **Criar uma Conta no Databricks**

1. Acesse [Databricks Community Edition](https://community.cloud.databricks.com/) (gr√°tis para testes) ou crie uma conta paga em [Databricks AWS/Azure](https://databricks.com/try-databricks).
2. Ap√≥s criar a conta, fa√ßa login no **Databricks Workspace**.

---

### 2Ô∏è‚É£ **Criar um Cluster no Databricks**

1. No menu esquerdo, clique em **Compute** ‚Üí **Create Cluster**.
2. Escolha um nome para o cluster (ex.: `pyspark-cluster`).
3. Selecione a vers√£o do **Databricks Runtime** (recomendo a mais recente com suporte ao PySpark).
4. Escolha um tipo de m√°quina adequado para os testes (ex.: `Single Node` na vers√£o Community ou `Standard` em AWS/Azure).
5. Clique em **Create Cluster** e aguarde a inicializa√ß√£o.

---

### 3Ô∏è‚É£ **Criar e Executar um Notebook**

1. No menu esquerdo, clique em **Workspace** ‚Üí **Users** ‚Üí Selecione seu usu√°rio.
2. Clique com o bot√£o direito e selecione **Create** ‚Üí **Notebook**.
3. Escolha um nome (ex.: `PySpark_Test`) e selecione a linguagem **Python**.
4. Associe o notebook ao cluster criado anteriormente.
5. Execute o c√≥digo abaixo no notebook:

```python
from pyspark.sql import SparkSession

spark = SparkSession.builder.appName("DatabricksTest").getOrCreate()

# Criar um DataFrame de teste
data = [(1, "Alice", 29), (2, "Bob", 35), (3, "Cathy", 32)]
columns = ["id", "name", "age"]
df = spark.createDataFrame(data, columns)

df.show()
```

Se o c√≥digo rodar corretamente e exibir a sa√≠da, seu ambiente PySpark no Databricks est√° pronto! üéâ

---

### 4Ô∏è‚É£ **Carregar um Arquivo CSV no Databricks**

1. No menu esquerdo, clique em **Data** ‚Üí **Add Data**.
2. Fa√ßa o upload de um arquivo CSV (ex.: `exemplo.csv`).
3. No notebook, use o seguinte c√≥digo para carregar e visualizar os dados:

```python
file_path = "/FileStore/tables/exemplo.csv"
df = spark.read.csv(file_path, header=True, inferSchema=True)
df.show()
```

---

## üî• **Integra√ß√£o com AWS S3, Azure Blob e GCP**

Para conectar o Databricks a armazenamentos na nuvem, configure credenciais e utilize comandos como:

### **AWS S3**
```python
df = spark.read.csv("s3://meu-bucket/dados.csv", header=True, inferSchema=True)
df.show()
```

### **Azure Blob Storage**
```python
spark.conf.set("fs.azure.account.key.meustorage.blob.core.windows.net", "<CHAVE>")
df = spark.read.csv("wasbs://meu-container@meustorage.blob.core.windows.net/dados.csv", header=True, inferSchema=True)
df.show()
```

### **Google Cloud Storage (GCS)**
```python
df = spark.read.csv("gs://meu-bucket/dados.csv", header=True, inferSchema=True)
df.show()
```

---

## üìÑ **Notas Finais**

- O Databricks Community Edition tem recursos limitados, mas √© suficiente para aprendizado.
- Clusters pagos oferecem mais poder computacional e armazenamento distribu√≠do.
- Integra-se facilmente com **AWS, Azure e GCP**.

Para mais detalhes, consulte a [documenta√ß√£o oficial do Databricks](https://docs.databricks.com/).

