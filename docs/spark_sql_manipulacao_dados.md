# Spark SQL e Manipula√ß√£o de Dados

Nesta se√ß√£o, exploramos como utilizar o **Spark SQL** para consultas em tabelas e manipula√ß√£o de dados estruturados no PySpark.

---

## üî• **O que √© Spark SQL?**

- O **Spark SQL** permite executar consultas SQL diretamente no PySpark, facilitando a manipula√ß√£o de dados estruturados.
- Ele suporta tanto a API de DataFrames quanto consultas SQL padr√£o.
- Pode ser integrado com bancos de dados, Data Lakes e outras fontes de dados estruturadas.

---

## üõ† **Cria√ß√£o e Manipula√ß√£o de Tabelas Tempor√°rias**

### 1Ô∏è‚É£ **Registrar um DataFrame como Tabela Tempor√°ria**
- Permite que um DataFrame seja consultado como uma tabela SQL.

```python
from pyspark.sql import SparkSession

# Criar SparkSession
spark = SparkSession.builder.appName("SparkSQLExample").getOrCreate()

# Criar um DataFrame
data = [(1, "Ana", 25), (2, "Carlos", 30), (3, "Jo√£o", 45)]
columns = ["id", "nome", "idade"]
df = spark.createDataFrame(data, columns)

# Registrar DataFrame como uma tabela tempor√°ria
df.createOrReplaceTempView("pessoas")
```

Agora, podemos executar consultas SQL diretamente:

```python
# Consultar os dados com SQL
resultado = spark.sql("SELECT * FROM pessoas")
resultado.show()
```

---

## üîó **Filtragem, Joins e Agrega√ß√µes com SQL**

### 1Ô∏è‚É£ **Filtragem de Dados**
Podemos usar SQL para aplicar filtros nos dados:

```python
# Selecionar pessoas com idade maior que 30
resultado = spark.sql("SELECT * FROM pessoas WHERE idade > 30")
resultado.show()
```

### 2Ô∏è‚É£ **Joins entre Tabelas**
Podemos combinar tabelas com diferentes tipos de joins:

```python
# Criar um segundo DataFrame
df_salarios = spark.createDataFrame([(1, 3000), (2, 4000), (3, 5000)], ["id", "salario"])
df_salarios.createOrReplaceTempView("salarios")

# Executar um JOIN entre as tabelas
resultado = spark.sql("""
    SELECT p.id, p.nome, p.idade, s.salario
    FROM pessoas p
    INNER JOIN salarios s
    ON p.id = s.id
""")
resultado.show()
```

### 3Ô∏è‚É£ **Agrega√ß√µes com SQL**
Consultas para sumarizar e agregar dados:

```python
# Calcular a idade m√©dia
resultado = spark.sql("SELECT AVG(idade) AS idade_media FROM pessoas")
resultado.show()
```

```python
# Contar o n√∫mero de registros
resultado = spark.sql("SELECT COUNT(*) AS total_pessoas FROM pessoas")
resultado.show()
```

---

## ‚ú® **Fun√ß√µes Embutidas e UDFs no Spark SQL**

### 1Ô∏è‚É£ **Uso de Fun√ß√µes SQL Embutidas**
Podemos usar fun√ß√µes nativas do Spark SQL:

```python
# Converter nomes para mai√∫sculas
resultado = spark.sql("SELECT id, UPPER(nome) AS nome_maiusculo FROM pessoas")
resultado.show()
```

### 2Ô∏è‚É£ **Criar e Usar UDFs em SQL**
Podemos definir fun√ß√µes personalizadas e utiliz√°-las nas consultas:

```python
from pyspark.sql.functions import udf
from pyspark.sql.types import StringType

# Criar uma fun√ß√£o personalizada
def prefixar_nome(nome):
    return f"Sr(a). {nome}"

# Registrar a fun√ß√£o como UDF
prefixar_udf = udf(prefixar_nome, StringType())
spark.udf.register("prefixar_nome", prefixar_udf)

# Aplicar a UDF na consulta SQL
resultado = spark.sql("SELECT id, prefixar_nome(nome) AS nome_formatado FROM pessoas")
resultado.show()
```

---

## üìÑ **Conclus√£o**

- O **Spark SQL** facilita a manipula√ß√£o de dados estruturados usando SQL dentro do PySpark.
- Podemos registrar DataFrames como tabelas tempor√°rias e executar consultas diretamente.
- Joins, agrega√ß√µes e fun√ß√µes embutidas s√£o poderosas para an√°lise de dados.
- UDFs permitem a cria√ß√£o de fun√ß√µes personalizadas dentro de consultas SQL.

Para mais informa√ß√µes, consulte a [documenta√ß√£o oficial do Spark SQL](https://spark.apache.org/docs/latest/sql-programming-guide.html).

