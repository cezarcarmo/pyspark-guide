# Fundamentos do PySpark: RDDs

Neste documento, exploramos os conceitos fundamentais dos **RDDs (Resilient Distributed Datasets)** no PySpark. Os RDDs s√£o a estrutura b√°sica de dados do Spark, permitindo processamento distribu√≠do de grandes volumes de dados.

---

## üî• **O que √© um RDD?**

- Um **RDD (Resilient Distributed Dataset)** √© uma cole√ß√£o distribu√≠da de objetos imut√°veis que podem ser processados em paralelo.
- RDDs s√£o particionados em v√°rios n√≥s do cluster, permitindo alto desempenho.
- S√£o tolerantes a falhas, pois o Spark pode recriar automaticamente os dados perdidos a partir das transforma√ß√µes aplicadas.

---

## üõ† **Criando um RDD no PySpark**

Para criar um RDD, utilizamos o `SparkContext`, que √© a base para interagir com o cluster do Spark.

### 1Ô∏è‚É£ **Criando um RDD a partir de uma lista**

```python
from pyspark.sql import SparkSession

# Criar SparkSession
spark = SparkSession.builder.appName("RDDExample").getOrCreate()
sc = spark.sparkContext  # Criar o SparkContext

# Criando um RDD a partir de uma lista
data = [1, 2, 3, 4, 5]
rdd = sc.parallelize(data)

# Exibir os elementos do RDD
print("Elementos do RDD:", rdd.collect())
```

### 2Ô∏è‚É£ **Criando um RDD a partir de um arquivo**

Podemos carregar um arquivo de texto como um RDD:

```python
rdd_from_file = sc.textFile("data/exemplo.txt")
print("Primeiras linhas do RDD:", rdd_from_file.take(5))
```

---

## üöÄ **Transforma√ß√µes e A√ß√µes em RDDs**

Os RDDs no PySpark operam com dois tipos de opera√ß√µes:

### **üîπ Transforma√ß√µes (Lazy Evaluation)**
- As transforma√ß√µes s√£o opera√ß√µes que definem um pipeline de processamento e s√≥ s√£o executadas quando uma **a√ß√£o** √© chamada.
- Exemplos de transforma√ß√µes: `map()`, `filter()`, `flatMap()`, `reduceByKey()`.

**Exemplo:**
```python
# Aplicando uma transforma√ß√£o para dobrar os valores
doubled_rdd = rdd.map(lambda x: x * 2)
print("RDD ap√≥s transforma√ß√£o:", doubled_rdd.collect())
```

### **üîπ A√ß√µes**
- As a√ß√µes executam as opera√ß√µes no RDD e retornam um valor.
- Exemplos de a√ß√µes: `collect()`, `count()`, `take()`, `reduce()`.

**Exemplo:**
```python
# Contando o n√∫mero de elementos no RDD
print("N√∫mero de elementos no RDD:", rdd.count())

# Somando todos os valores do RDD
soma = rdd.reduce(lambda x, y: x + y)
print("Soma dos elementos do RDD:", soma)
```

---

## üîÑ **Particionamento e Otimiza√ß√£o de RDDs**

O particionamento impacta diretamente a performance do PySpark. Podemos definir o n√∫mero de parti√ß√µes ao criar um RDD:

```python
rdd_particionado = sc.parallelize(data, numSlices=3)
print("N√∫mero de parti√ß√µes:", rdd_particionado.getNumPartitions())
```

O particionamento correto evita **sobrecarga de mem√≥ria** e melhora o desempenho em clusters distribu√≠dos.

---

## üìÑ **Conclus√£o**

- RDDs s√£o a estrutura de dados mais fundamental do PySpark.
- Transforma√ß√µes s√£o **lazy**, ou seja, s√£o aplicadas apenas quando uma a√ß√£o √© chamada.
- O particionamento eficiente melhora a performance e escalabilidade do processamento de dados.

Para mais detalhes, consulte a [documenta√ß√£o oficial do Apache Spark](https://spark.apache.org/docs/latest/rdd-programming-guide.html).