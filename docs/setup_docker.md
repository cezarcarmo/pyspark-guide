# Configura√ß√£o do PySpark com Docker

Nesta se√ß√£o, configuraremos um ambiente PySpark utilizando **Docker**, permitindo um ambiente isolado e port√°til para execu√ß√£o de pipelines de dados.

---

## üìå **Pr√©-requisitos**

1. **Docker instalado**:
   - Verifique se o Docker est√° instalado executando:
     ```bash
     docker --version
     ```
   - Se n√£o estiver instalado, siga as instru√ß√µes do [Docker Oficial](https://docs.docker.com/get-docker/).

2. **Docker Compose (opcional, mas recomendado)**:
   - Verifique se est√° instalado:
     ```bash
     docker-compose --version
     ```
   - Se necess√°rio, instale via:
     ```bash
     pip install docker-compose
     ```

---

## üöÄ **Configurando o Ambiente PySpark com Docker**

### 1Ô∏è‚É£ **Criar um `Dockerfile`**
No diret√≥rio raiz do projeto, crie um arquivo chamado `Dockerfile` e adicione o seguinte conte√∫do:

```dockerfile
# Usar imagem oficial do Spark
FROM bitnami/spark:3.4.0

# Definir vari√°veis de ambiente
ENV SPARK_HOME=/opt/spark
ENV PATH="$SPARK_HOME/bin:$PATH"

# Criar diret√≥rio para scripts
WORKDIR /app
COPY scripts /app/scripts

# Definir comando padr√£o
CMD ["pyspark"]
```

---

### 2Ô∏è‚É£ **Criar um `docker-compose.yml`**
Se quiser gerenciar o ambiente facilmente, crie um `docker-compose.yml` com o seguinte conte√∫do:

```yaml
version: '3'
services:
  spark:
    image: bitnami/spark:3.4.0
    container_name: pyspark-container
    environment:
      - SPARK_HOME=/opt/spark
    volumes:
      - ./scripts:/app/scripts
    working_dir: /app
    command: ["pyspark"]
    ports:
      - "4040:4040"  # Porta da interface do Spark UI
```

---

### 3Ô∏è‚É£ **Construir e Executar o Container**

1. **Construir a imagem Docker:**
   ```bash
   docker build -t pyspark-image .
   ```

2. **Executar o container diretamente:**
   ```bash
   docker run --rm -it -v $(pwd)/scripts:/app/scripts pyspark-image
   ```

3. **Ou executar com Docker Compose:**
   ```bash
   docker-compose up -d
   ```
   - Para acessar o container:
     ```bash
     docker exec -it pyspark-container bash
     ```
   - Para interromper o ambiente:
     ```bash
     docker-compose down
     ```

---

## üî• **Testando o PySpark dentro do Container**

Ap√≥s iniciar o container, teste o PySpark rodando:

```bash
docker exec -it pyspark-container pyspark
```

Dentro do shell interativo, rode:

```python
from pyspark.sql import SparkSession

spark = SparkSession.builder.appName("DockerTest").getOrCreate()
print(spark.version)
```

Se a vers√£o do Spark for exibida corretamente, o ambiente est√° configurado! üéâ

---

## üìÑ **Notas Finais**

- A interface web do Spark UI pode ser acessada via `http://localhost:4040`.
- Para persistir dados, configure volumes adicionais no `docker-compose.yml`.
- Se precisar de mais n√≥s, considere configurar um **cluster Spark com Docker Swarm ou Kubernetes**.

Para mais informa√ß√µes, consulte a [documenta√ß√£o oficial do Spark](https://spark.apache.org/docs/latest/).

