# Compara√ß√£o: Pandas vs PySpark

**Pandas** e **PySpark** s√£o ferramentas populares para manipula√ß√£o de dados, mas foram projetadas para cen√°rios diferentes. Abaixo est√° uma compara√ß√£o detalhada que pode ajudar a escolher qual ferramenta utilizar em diferentes contextos.

---

## üîë **Diferen√ßas Fundamentais**

| Caracter√≠stica               | Pandas                                   | PySpark                                  |
|------------------------------|------------------------------------------|------------------------------------------|
| **Volume de Dados**          | Funciona bem com dados que cabem na mem√≥ria local. | Projetado para lidar com grandes volumes de dados em clusters. |
| **Processamento**            | Processamento em um √∫nico n√≥ (local).   | Processamento distribu√≠do em m√∫ltiplos n√≥s.                   |
| **Velocidade**               | R√°pido para pequenos datasets.          | Escal√°vel e eficiente para grandes datasets.                  |
| **API**                      | Simples e intuitiva, baseado em Python. | Similar ao Pandas, mas com suporte a opera√ß√µes distribu√≠das.   |
| **Integra√ß√£o**               | Ideal para an√°lises locais e pipelines menores. | Integra-se com Hadoop, Spark SQL, MLlib, etc.                |
| **Casos de Uso**             | Explora√ß√£o e pr√©-processamento de dados em m√°quinas locais. | ETL, an√°lise de Big Data e aprendizado de m√°quina em larga escala. |

---

## üöÄ **Exemplo Pr√°tico: Compara√ß√£o em C√≥digo**

### 1Ô∏è‚É£ **Leitura de Dados**

#### Usando Pandas:
```python
import pandas as pd

# Ler um arquivo CSV
file_path = "data/exemplo.csv"
df = pd.read_csv(file_path)
print(df.head())
```

#### Usando PySpark:
```python
from pyspark.sql import SparkSession

# Criar uma SparkSession
spark = SparkSession.builder.appName("ExemploPySpark").getOrCreate()

# Ler um arquivo CSV
file_path = "data/exemplo.csv"
df = spark.read.csv(file_path, header=True, inferSchema=True)
df.show(5)
```

---

### 2Ô∏è‚É£ **Opera√ß√µes B√°sicas**

#### Filtro por Condi√ß√£o

**Pandas:**
```python
# Filtrar linhas onde a idade √© maior que 30
filtered_df = df[df['idade'] > 30]
print(filtered_df)
```

**PySpark:**
```python
# Filtrar linhas onde a idade √© maior que 30
filtered_df = df.filter(df.idade > 30)
filtered_df.show()
```

---

### 3Ô∏è‚É£ **Agrega√ß√µes**

**Pandas:**
```python
# Calcular a m√©dia de idade por grupo
mean_age = df.groupby('grupo')['idade'].mean()
print(mean_age)
```

**PySpark:**
```python
# Calcular a m√©dia de idade por grupo
df.groupBy("grupo").avg("idade").show()
```

---

## üìä **Quando Usar Cada Ferramenta**

| Cen√°rio                               | Ferramenta Recomendada |
|---------------------------------------|-------------------------|
| Dados pequenos, an√°lise r√°pida        | Pandas                 |
| Processamento de grandes volumes de dados | PySpark                |
| Integra√ß√£o com ferramentas de Big Data | PySpark                |
| An√°lise local e prototipagem          | Pandas                 |

---

## üî• **Conclus√£o**

- **Pandas** √© ideal para an√°lises locais r√°pidas e prototipagem.
- **PySpark** √© a escolha certa para grandes volumes de dados e cen√°rios distribu√≠dos.

Escolha a ferramenta com base no tamanho do dataset e no contexto do seu projeto!

