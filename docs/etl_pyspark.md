# ETL com PySpark

Nesta se√ß√£o, exploramos como construir um **pipeline ETL (Extract, Transform, Load)** usando PySpark para processar dados estruturados e semi-estruturados.

---

## üî• **O que √© um Pipeline ETL?**

- **Extract (Extra√ß√£o)**: Leitura de dados de diferentes fontes, como arquivos CSV, JSON, Parquet e bancos de dados SQL.
- **Transform (Transforma√ß√£o)**: Limpeza, enriquecimento e agrega√ß√£o dos dados.
- **Load (Carga)**: Escrita dos dados transformados em diferentes formatos e destinos.

---

## üì• **Extra√ß√£o de Dados**

### 1Ô∏è‚É£ **Ler Arquivos CSV, JSON e Parquet**

```python
from pyspark.sql import SparkSession

# Criar SparkSession
spark = SparkSession.builder.appName("ETL_PySpark").getOrCreate()

# Ler um arquivo CSV
df_csv = spark.read.csv("data/exemplo.csv", header=True, inferSchema=True)

# Ler um arquivo JSON
df_json = spark.read.json("data/exemplo.json")

# Ler um arquivo Parquet
df_parquet = spark.read.parquet("data/exemplo.parquet")
```

### 2Ô∏è‚É£ **Leitura de Dados de um Banco SQL via JDBC**

```python
jdbc_url = "jdbc:mysql://localhost:3306/meu_banco"
properties = {"user": "root", "password": "senha", "driver": "com.mysql.cj.jdbc.Driver"}

df_sql = spark.read.jdbc(url=jdbc_url, table="tabela_exemplo", properties=properties)
df_sql.show()
```

---

## üîÑ **Transforma√ß√£o dos Dados**

### 1Ô∏è‚É£ **Selecionar e Renomear Colunas**
```python
df_transformado = df_csv.selectExpr("id", "nome", "idade + 1 as idade_atualizada")
df_transformado.show()
```

### 2Ô∏è‚É£ **Filtrar e Limpar Dados**
```python
# Remover valores nulos
df_limpo = df_csv.dropna()

# Filtrar registros espec√≠ficos
df_filtrado = df_csv.filter(df_csv["idade"] > 30)
df_filtrado.show()
```

### 3Ô∏è‚É£ **Agrega√ß√£o e Enriquecimento**
```python
from pyspark.sql.functions import avg, count

# Calcular idade m√©dia
df_media = df_csv.groupBy("cidade").agg(avg("idade").alias("media_idade"))
df_media.show()
```

---

## üì§ **Carga dos Dados**

### 1Ô∏è‚É£ **Salvar Dados Processados em Arquivos**
```python
# Salvar como CSV
df_transformado.write.csv("output/dados_transformados.csv", header=True)

# Salvar como Parquet
df_transformado.write.parquet("output/dados_transformados.parquet")
```

### 2Ô∏è‚É£ **Escrever Dados Processados em um Banco SQL**
```python
df_transformado.write.jdbc(url=jdbc_url, table="tabela_processada", mode="overwrite", properties=properties)
```

---

## üìÑ **Conclus√£o**

- O PySpark permite construir pipelines ETL escal√°veis e eficientes.
- Podemos extrair dados de m√∫ltiplas fontes, transform√°-los aplicando filtros e agrega√ß√µes, e carreg√°-los em diferentes destinos.
- Integra-se facilmente com bancos de dados e formatos de armazenamento modernos.

Para mais detalhes, consulte a [documenta√ß√£o oficial do PySpark](https://spark.apache.org/docs/latest/sql-data-sources.html).

