# Guia Completo de PySpark para Engenharia de Dados

## ğŸ“Œ Objetivo
Este repositÃ³rio tem como objetivo fornecer um guia completo e prÃ¡tico sobre o uso do **PySpark** na Engenharia de Dados. O projeto abrange desde conceitos fundamentais atÃ© implementaÃ§Ãµes avanÃ§adas, incluindo **Spark SQL** e integraÃ§Ã£o com diversas ferramentas.

## ğŸ“‚ Estrutura do RepositÃ³rio
```
ğŸ“‚ pyspark-guide/
â”‚â”€â”€ ğŸ“‚ notebooks/         # Notebooks interativos com exemplos prÃ¡ticos
â”‚â”€â”€ ğŸ“‚ scripts/           # Scripts PySpark organizados para execuÃ§Ã£o modular
â”‚â”€â”€ ğŸ“‚ data/              # Conjunto de dados para os exemplos prÃ¡ticos
â”‚â”€â”€ ğŸ“‚ docs/              # DocumentaÃ§Ã£o explicativa e guia de estudo
â”‚â”€â”€ ğŸ“‚ tests/             # Testes para validaÃ§Ã£o dos pipelines
â”‚â”€â”€ ğŸ“œ README.md          # DescriÃ§Ã£o do projeto
â”‚â”€â”€ ğŸ“œ requirements.txt   # DependÃªncias do projeto
â”‚â”€â”€ ğŸ“œ setup.py           # ConfiguraÃ§Ã£o do ambiente
```

## ğŸ”¥ ConteÃºdo do Projeto
### 1ï¸âƒ£ IntroduÃ§Ã£o ao PySpark
- O que Ã© PySpark?
  - PySpark Ã© a API em Python do Apache Spark, um framework de processamento de dados distribuÃ­dos projetado para grandes volumes de informaÃ§Ãµes. Ele permite criar aplicaÃ§Ãµes escalÃ¡veis para processamento de dados em clusters. Para mais detalhes, consulte [O que Ã© PySpark?](docs/what_is_pyspark.md).
- InstalaÃ§Ã£o e configuraÃ§Ã£o no VSCode
- ConfiguraÃ§Ã£o de ambiente local: Standalone, Docker e Databricks.
  - Para um guia detalhado sobre a ConfiguraÃ§Ã£o do PySpark em Modo [Standalone](docs/setup_standalone.md).
  - Para um guia detalhado sobre a ConfiguraÃ§Ã£o do PySpark com [Docker](docs/setup_docker.md).
  - Para um guia detalhado sobre a ConfiguraÃ§Ã£o do PySpark no [Databricks](docs/setup_databricks.md).
- ComparaÃ§Ã£o: Pandas vs PySpark
  - Para um guia detalhado sobre as diferenÃ§as, veja [Pandas vs PySpark](docs/pyspark_vs_pandas.md).

### 2ï¸âƒ£ Fundamentos do PySpark
- Estrutura do **SparkSession** e **RDD**
- DataFrames no PySpark: Criando e manipulando
- TransformaÃ§Ãµes e AÃ§Ãµes no PySpark
- PartiÃ§Ãµes e otimizaÃ§Ã£o de performance

### 3ï¸âƒ£ TransformaÃ§Ãµes e AÃ§Ãµes em DataFrames
- **TransformaÃ§Ãµes:**
  - Adicionar colunas com `withColumn`.
  - Remover colunas com `drop`.
  - Selecionar valores Ãºnicos com `distinct`.
  - Renomear colunas.
- **AÃ§Ãµes:**
  - Contar registros com `count`.
  - Coletar dados localmente com `collect`.
  - Exibir registros com `show` e `take`.

### 4ï¸âƒ£ FunÃ§Ãµes AvanÃ§adas em DataFrames
- **Agrupamento e AgregaÃ§Ãµes:**
  - Agrupar dados com `groupBy`.
  - Aplicar funÃ§Ãµes de agregaÃ§Ã£o como `sum`, `avg`, e `max`.
- **Joins:**
  - Combinar DataFrames com diferentes tipos de joins (`inner`, `left`, `right`, `outer`).
- **UDFs (User Defined Functions):**
  - Criar e aplicar funÃ§Ãµes personalizadas para manipular colunas de DataFrames.

### 5ï¸âƒ£ Spark SQL e ManipulaÃ§Ã£o de Dados
- Criando e manipulando tabelas com Spark SQL
- Filtragem, Joins e AgregaÃ§Ãµes
- FunÃ§Ãµes embutidas no PySpark SQL
- Criando funÃ§Ãµes UDFs no PySpark

### 6ï¸âƒ£ ETL com PySpark
- Lendo dados de diferentes fontes (**CSV, Parquet, JSON, JDBC, Delta Lake**)
- Escrevendo dados em diferentes formatos e destinos
- TransformaÃ§Ãµes e tratamentos de dados
- Pipeline ETL completo com PySpark

### 7ï¸âƒ£ OtimizaÃ§Ã£o e Performance
- Gerenciamento de PartiÃ§Ãµes
- Broadcast Joins e Cache
- Uso de Catalyst Optimizer
- Melhores prÃ¡ticas para performance em PySpark

### 8ï¸âƒ£ IntegraÃ§Ã£o com Cloud (AWS, GCP, Azure)
- Executando PySpark na AWS Glue
- Rodando PySpark no Databricks
- IntegraÃ§Ã£o com **S3, BigQuery, ADLS**

### 9ï¸âƒ£ Casos PrÃ¡ticos de Engenharia de Dados
- Pipeline de processamento de logs
- AnÃ¡lise de dados de e-commerce
- Processamento de grandes volumes de dados com PySpark

## ğŸ›  Tecnologias Utilizadas
âœ… PySpark  
âœ… Spark SQL  
âœ… AWS Glue (opcional)  
âœ… Databricks (opcional)  
âœ… Jupyter Notebook  
âœ… Docker (para testes locais)  
âœ… VSCode + Git  

## ğŸš€ Como ComeÃ§ar
1. Clone o repositÃ³rio:
   ```bash
   git clone https://github.com/seu-usuario/pyspark-guide.git
   cd pyspark-guide
   ```
2. Instale as dependÃªncias:
   ```bash
   pip install -r requirements.txt
   ```
3. Execute os scripts localizados no diretÃ³rio `scripts/`:
   ```bash
   python scripts/<nome_do_script>.py
   ```
4. Exemplos disponÃ­veis:
   - **RDDs e DataFrames:**
     ```bash
     python scripts/rdd_example.py
     ```
   - **TransformaÃ§Ãµes e AÃ§Ãµes:**
     ```bash
     python scripts/dataframe_transformacoes_acoes_script.py
     ```
   - **FunÃ§Ãµes AvanÃ§adas:**
     ```bash
     python scripts/dataframe_funcoes_avancadas_script.py
     ```

## ğŸ“Œ ContribuiÃ§Ã£o
Fique Ã  vontade para abrir **issues** e **pull requests** para melhorias no projeto.

## ğŸ“„ LicenÃ§a
Este projeto estÃ¡ sob a licenÃ§a MIT.
